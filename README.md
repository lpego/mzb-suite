# Macrozoobenthos processing suite: mzb-workflow

This project is in very early development. It is a work in progress and is not yet ready for use. 

Full workflow for the Macrozoobenthos pipeline: 
- Parse raw data from the Macrozoobenthos database
- Clean and process data, both raw and manually annotated ones
- Set up a data folder structure that is easy to navigate and understand, and that can be used for further analysis and train models contained and devined in this project
- It mostly implements experiments for the project `portable_flume`, but it can be easily adapted to other projects. This is a TODO. 

## Introduction

This project contains several modules, but it mainly implements 3 modules: 
1 - _classification_: this repo contains the code to train and test a model to classify the macrozoobenthos samples, according to a a set of predefined classes. In our experiments, image classes were specified in the filename, but this can be changed to a more flexible approach. We reclassify class names thanks to the `data/MZB_taxonomy.csv` file, which groups classes according to a taxonomic hierarchy. The data used to fine tune pretrained deep learning classifiers is then duplicated according to our structure (each class in its own folder) in `data/learning_sets/project_portable_flume/aggregated_learning_sets/`.

2 - _skeletonization_ and _size measurements_: this repo contains the code to measure the length and width of macrozoobenthos samples. The user can chose two approaches: one is completely unupservised, and uses image filtering, mathematical morphology and graph-traversal algorithm to come up with a measure approximating _only the length_ of the sample. Performance is better for samples that are long and thig, and possibly less accurate for complex shapes. The apprioximation for this approach is made from a binary mask obtained automatically from the MZB sample. 
The second approach to measure size is based on supervised deep learning models, which are trained based on some manual annotations of insects length (head to tail) and width (width of head). Those annotations were provided by a human expert. The scripts in this repo allow to finetune this model, and to use this model for inference on new images. The two predictions of this model correspond to the probability of each pixel to be along the "body" skeleton segments, or along the "head" skeleton segments. Postprocessings thin those predictions to a line, and the size of the sample is then approximated by the sum of the length of the body and the head.

3 - _area and instance segmentation_: These are mainly used as preprocessing steps for classification and measurements, but are handy functionalities on their own. The raw input images are wide RGB photographs of many macrozoobenthos samples, which are hard to process on their own. A first step is to detect all the insects as independent and disjoint objects, and to crop them out of the original image. This is done with traditional image processing methods, and it is completely unsupervised. From this, we derive i) an RGB crop of the insect, filename corresponds to the original image name plus clip ID and ii) a binary mask of the insect, filename corresponds to the original image name plus clip ID. The binary mask can be used to compute the area of the insect (number of pixels) and RGB + clip can be used to compute local descriptors (we can think of using classificaiton models that are not convolutional neural nets, such as random forests, to classify the insect based on these local color and shape descriptors).

## Working with the project

### Project structure

- `data/`: contains the raw data, and the some of the data generated by scripts, that is then reused elswhere (eg image clips -- aka blobs, masks, skeletons, etc). Some CSV with derived measures and features are momentarily stored there but might not always be needed. We can maybe generate those conditionally by addition option in `configs/global_configuration.yaml`.
- `mzb_workflow/`: contains the utility functions for each module, that one can modify for specific needs, but are in general good and generic enough to be used as it is.
- `scripts/`: contains the scripts to run the different modules. Each module has its own folder, and each script is named after the module it implements. These modules can _be run in isolation_, ie, each one of those can be run independently from the others from the CLI. Arguments to these scripts are only the paths to the input and output folders, and the name of the project, model names. This is made so renku workflow can track main inputs and outputs (TODOs) without making it complex. A user might just need to specify folder names of new data on which to perform inference, and that's all. 
Some work might be needed to ensure that they can also be run interactively, but for this, it is better to duplicate those fiels in a `notebooks/` folde, and to run them interactively from there after modding them. 
- `models/`: contains the pretrained models used in the different modules. The models are first downloaded from pytorch model zoo, then finetuned. The finetuned models are saved in this folder.
- `configs/`: contains for now one big config file, `config.yaml`, which contains all the parameters for the different modules. This module contains all defaults settings and hyperparameters, and it can be modified by the user to change the behavior of the scripts. Maybe good idea to create copies of this config, with per-experiement naming, or create a branch of the repo, etc. Make sure to version those also, with good commit names. Those config files are always input to main scripts. 
- `results/`: contains the results of the different modules. Each module has its own folder, and each script is named after the module it implements. It probably needs to be better structured and organized. 
- `workflows/`: will contain the renku workflows, for now contains just an implementation of the serial pipeline in bash scripts. One for the inference pipeline, and two for finetining the classification and size measurement models.

### Workflow files

In this folder, there are "workflow" files that can be used to run the pipeline. Those files are nothing else that a chain of python commands implementing the flow of the processing pipeline. For instance, just run `./workflows/run_finetune_skeletonization.sh` to fine tune the skeletonization model. 

TODO: transfer those bash scripts to renku workflows, so that the renku can track the dependencies and the inputs and outputs of each step, and generate the graph of the workflow.

### Changing interactive session dependencies

The full conda env is given in `environment.yml`. We need to check if the docker image builds...

## Changelog 

**v0.1.0** First release. Attempt to structure project and scripts in a way that is easy to understand and maintain.
