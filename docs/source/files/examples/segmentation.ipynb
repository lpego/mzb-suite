{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image segmentation\n",
    "==================\n",
    "\n",
    "In this notebook we illustrate how to use the script `scripts/image_parsing/main_raw_to_clips.py` to segment (i.e. extract) clips containing a single organisms from large-pane images containing multiple organisms. \n",
    "\n",
    "As a first, step, import the necessary packages, including the custom functions of this repository `msbsuite.utils` (if you have trouble importing this package, refer back to the `Installation` section of the documentation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import ndimage\n",
    "from skimage import feature, measure, morphology, segmentation\n",
    "from tqdm import tqdm\n",
    "\n",
    "from mzbsuite.utils import cfg_to_arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to declare the parameters to tell the script where to find the files and where to save its outputs. In this notebook, we pass these arguments as a dictionary to Python, rather than variables in a shell (`.sh`) script. \n",
    "\n",
    "You need to have downloaded the example dataset in order for this cell to compile properly. Alternatively you can change the file paths to the locations of folders of your own dataset on the `arguments = {}` block; the path is relative to where this notebook is located. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'glob_random_seed': 222,\n",
       " 'glob_root_folder': '/home/jovyan/work/mzb-workflow/',\n",
       " 'glob_blobs_folder': '/home/jovyan/work/mzb-workflow/data/derived/blobs/',\n",
       " 'glob_local_format': 'pdf',\n",
       " 'model_logger': 'wandb',\n",
       " 'impa_image_format': 'jpg',\n",
       " 'impa_clip_areas': [2700, 4700, -1, -1],\n",
       " 'impa_area_threshold': 5000,\n",
       " 'impa_gaussian_blur': [21, 21],\n",
       " 'impa_gaussian_blur_passes': 3,\n",
       " 'impa_adaptive_threshold_block_size': 351,\n",
       " 'impa_mask_postprocess_kernel': [11, 11],\n",
       " 'impa_mask_postprocess_passes': 5,\n",
       " 'impa_bounding_box_buffer': 200,\n",
       " 'impa_save_clips_plus_features': True,\n",
       " 'lset_class_cut': 'order',\n",
       " 'lset_val_size': 0.1,\n",
       " 'trcl_learning_rate': 0.0001,\n",
       " 'trcl_batch_size': 8,\n",
       " 'trcl_weight_decay': 0,\n",
       " 'trcl_step_size_decay': 5,\n",
       " 'trcl_number_epochs': 75,\n",
       " 'trcl_save_topk': 1,\n",
       " 'trcl_num_classes': 8,\n",
       " 'trcl_model_pretrarch': 'convnext-small',\n",
       " 'trcl_num_workers': 16,\n",
       " 'trcl_wandb_project_name': 'mzb-classifiers',\n",
       " 'trcl_logger': 'wandb',\n",
       " 'trsk_learning_rate': 0.001,\n",
       " 'trsk_batch_size': 32,\n",
       " 'trsk_weight_decay': 0,\n",
       " 'trsk_step_size_decay': 25,\n",
       " 'trsk_number_epochs': 400,\n",
       " 'trsk_save_topk': 1,\n",
       " 'trsk_num_classes': 2,\n",
       " 'trsk_model_pretrarch': 'mit_b2',\n",
       " 'trsk_num_workers': 16,\n",
       " 'trsk_wandb_project_name': 'mzb-skeletons',\n",
       " 'trsk_logger': 'wandb',\n",
       " 'infe_model_ckpt': 'last',\n",
       " 'infe_num_classes': 8,\n",
       " 'infe_image_glob': '*_rgb.jpg',\n",
       " 'skel_class_exclude': 'errors',\n",
       " 'skel_conv_rate': 131.6625,\n",
       " 'skel_label_thickness': 3,\n",
       " 'skel_label_buffer_on_preds': 25,\n",
       " 'skel_label_clip_with_mask': False,\n",
       " 'trcl_gpu_ids': None}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROOT_DIR = Path(\"/data/shared/mzb-workflow\")\n",
    "\n",
    "arguments = {\n",
    "    \"input_dir\": ROOT_DIR / \"data/mzb_example_data/raw_img/\", \n",
    "    \"output_dir\": ROOT_DIR / \"data/derived/mzb_example_data/\", \n",
    "    \"save_full_mask_dir\": ROOT_DIR / \"data/derived/mzb_example_data/full_image_masks/\", \n",
    "    \"config_file\": ROOT_DIR / \"configs/mzb_example_config.yaml\", \n",
    "}\n",
    "    \n",
    "with open(str(arguments[\"config_file\"]), \"r\") as f:\n",
    "    cfg = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "cfg[\"trcl_gpu_ids\"] = None # this sets the number of available GPUs to zero, since this script doesn't benefit from GPU compute. \n",
    "cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cfg` variable should display a portion of the list of parameters in the configuration file. \n",
    "\n",
    "Now we use custom function `cfg_to_arguments` to parse the parameters we have just supplied and the parameters in the configuration file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'glob_random_seed': 222, 'glob_root_folder': '/home/jovyan/work/mzb-workflow/', 'glob_blobs_folder': '/home/jovyan/work/mzb-workflow/data/derived/blobs/', 'glob_local_format': 'pdf', 'model_logger': 'wandb', 'impa_image_format': 'jpg', 'impa_clip_areas': [2700, 4700, -1, -1], 'impa_area_threshold': 5000, 'impa_gaussian_blur': [21, 21], 'impa_gaussian_blur_passes': 3, 'impa_adaptive_threshold_block_size': 351, 'impa_mask_postprocess_kernel': [11, 11], 'impa_mask_postprocess_passes': 5, 'impa_bounding_box_buffer': 200, 'impa_save_clips_plus_features': True, 'lset_class_cut': 'order', 'lset_val_size': 0.1, 'trcl_learning_rate': 0.0001, 'trcl_batch_size': 8, 'trcl_weight_decay': 0, 'trcl_step_size_decay': 5, 'trcl_number_epochs': 75, 'trcl_save_topk': 1, 'trcl_num_classes': 8, 'trcl_model_pretrarch': 'convnext-small', 'trcl_num_workers': 16, 'trcl_wandb_project_name': 'mzb-classifiers', 'trcl_logger': 'wandb', 'trsk_learning_rate': 0.001, 'trsk_batch_size': 32, 'trsk_weight_decay': 0, 'trsk_step_size_decay': 25, 'trsk_number_epochs': 400, 'trsk_save_topk': 1, 'trsk_num_classes': 2, 'trsk_model_pretrarch': 'mit_b2', 'trsk_num_workers': 16, 'trsk_wandb_project_name': 'mzb-skeletons', 'trsk_logger': 'wandb', 'infe_model_ckpt': 'last', 'infe_num_classes': 8, 'infe_image_glob': '*_rgb.jpg', 'skel_class_exclude': 'errors', 'skel_conv_rate': 131.6625, 'skel_label_thickness': 3, 'skel_label_buffer_on_preds': 25, 'skel_label_clip_with_mask': False, 'trcl_gpu_ids': None}\n"
     ]
    }
   ],
   "source": [
    "args = cfg_to_arguments(arguments)\n",
    "cfg = cfg_to_arguments(cfg)\n",
    "print(str(cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check whether the output directories already exist, and if not create them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paths\n",
    "main_root = Path(args.input_dir)\n",
    "outdir = Path(args.output_dir)\n",
    "outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if args.save_full_mask_dir is not None:\n",
    "        args.save_full_mask_dir = Path(args.save_full_mask_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the contents of the input folder and standardise filenames, and print how many images are going to be processed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing 2 files\n"
     ]
    }
   ],
   "source": [
    "# get list of files to process\n",
    "files_proc = list(main_root.glob(f\"**/*.{cfg.impa_image_format}\"))\n",
    "# make sure weird capitalization doesn't cause issues\n",
    "files_proc.extend(list(main_root.glob(f\"**/*.{cfg.impa_image_format.upper()}\")))\n",
    "files_proc = [a for a in files_proc if \"mask\" not in str(a)]\n",
    "files_proc.sort()\n",
    "\n",
    "print(f\"Parsing {len(files_proc)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a clip area is defined, for instance if there is a reference scale in the same spot in all the images, this area is earmarked for exclusion in later processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.impa_clip_areas is not None:\n",
    "    location_cutout = [int(a) for a in cfg.impa_clip_areas]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the `PLOTS` variable is True, then the script will print out a summary plot for each image and for each individual clip being generated. If you don't want plots being generated, change the value to False. If you would like to save each plot as a file, you can uncomment (i.e. remove `# `) the lines `plt.savefig()` in the loop below. \n",
    "\n",
    "> ⚠️ WARNING: this can be computationally intensive and can potentially crash the notebook if a large number number of outputs is generated! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOTS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a normalisation function to flatten the pixel values of images (this helps with downstream processing). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define quick normalization function\n",
    "norm = lambda x: (x - np.min(x)) / (np.max(x) - np.min(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the main loop that processes the images into clips, and will also produce figures if `PLOTS = True`. If `PLOTS = False`, the script will save a `.csv` file with information about each image and clips generated from it, as well as other information such as bounding box coordinates, pixel areas of the mask, etc. \n",
    "\n",
    "For further details about the logic fo this script please refer to the explanation in the section `Segmentation` under `Processing scripts` in the documentation. \n",
    "\n",
    "> ⚠️ WARNING: depending on the number of images and how many organisms are present, the processing time of the loop can be considerable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### ATTEMPT AT UPDATING THE FIGURE IN-PLACE, INSTEAD OF GENERATING NEW FIGURES ALL THE TIME... \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1) \n",
    "\n",
    "for i in range(21):\n",
    "    ax.set_xlim(0, 20)\n",
    "    \n",
    "    ax.plot(i, 1, marker='x')\n",
    "    display(fig)\n",
    "    \n",
    "    clear_output(wait = True)\n",
    "    plt.pause(0.5)\n",
    "\n",
    "# ?display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Iteration 5 Score: 0.02057023479882769'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### EXPERIMENTING WITH clear_output... \n",
    "\n",
    "from random import uniform\n",
    "import time\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def black_box():\n",
    "    i = 1\n",
    "    while i <= 5:\n",
    "        clear_output(wait=True)\n",
    "        display('Iteration '+str(i)+' Score: '+str(uniform(0, 1)))\n",
    "        time.sleep(1)\n",
    "        i += 1\n",
    "\n",
    "black_box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [07:15<00:00, 217.80s/it]\n"
     ]
    }
   ],
   "source": [
    "iterator = tqdm(files_proc, total=len(files_proc))\n",
    "for i, fo in enumerate(iterator):\n",
    "    \n",
    "    mask_props = []\n",
    "\n",
    "    # get image path\n",
    "    raw_image_in = fo\n",
    "    full_path_raw_image_in = fo.resolve()\n",
    "\n",
    "    # read image and convert to HSV\n",
    "    img = cv2.imread(str(full_path_raw_image_in))[:, :, [2, 1, 0]]\n",
    "\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    im_t = hsv[:, :, 0].copy()\n",
    "    im_t = (255 * norm(np.mean(hsv[:, :, :2], axis=2))).astype(np.uint8)\n",
    "\n",
    "    # filter image with some iterations of gaussian blur\n",
    "    for _ in range(cfg.impa_gaussian_blur_passes):\n",
    "        im_t = cv2.GaussianBlur(im_t, tuple(cfg.impa_gaussian_blur), 0)\n",
    "\n",
    "    # prepare for morphological reconstruction\n",
    "    seed = np.copy(im_t)\n",
    "    seed[1:-1, 1:-1] = im_t.min()\n",
    "    mask = np.copy(im_t)\n",
    "\n",
    "    # remove the background\n",
    "    dil = morphology.reconstruction(seed, im_t, method=\"dilation\")\n",
    "    im_t = (im_t - dil).astype(np.uint8)\n",
    "\n",
    "    # adaptive local thresholding of foreground vs background\n",
    "    # weighted cross correlation with gaussian filter\n",
    "    ad_thresh = cv2.adaptiveThreshold(\n",
    "        im_t,\n",
    "        255,\n",
    "        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "        cv2.THRESH_BINARY,\n",
    "        cfg.impa_adaptive_threshold_block_size,\n",
    "        -2,\n",
    "    )\n",
    "    # additional global threhsold to remove foreground vs background\n",
    "    t, thresh = cv2.threshold(im_t, 0, 255, cv2.THRESH_OTSU)\n",
    "\n",
    "    # merge thresholds to globally get foreground masks\n",
    "    # thresh = thresh | ad_thresh\n",
    "    thresh = thresh + ad_thresh > 0\n",
    "\n",
    "    # postprocess masking to remove small objects and fill holes\n",
    "    kernel = np.ones(cfg.impa_mask_postprocess_kernel, np.uint8)\n",
    "    for _ in range(cfg.impa_mask_postprocess_passes):\n",
    "        thresh = cv2.morphologyEx(\n",
    "            (255 * thresh).astype(np.uint8), cv2.MORPH_CLOSE, kernel\n",
    "        )\n",
    "        thresh = cv2.morphologyEx(\n",
    "            (255 * thresh).astype(np.uint8), cv2.MORPH_OPEN, kernel\n",
    "        )\n",
    "    thresh = ndimage.binary_fill_holes(thresh)\n",
    "\n",
    "    # cut out area related to measurement/color calibration widget\n",
    "    if cfg.impa_clip_areas is not None:\n",
    "        thresh[\n",
    "            location_cutout[0] : location_cutout[2],\n",
    "            location_cutout[1] : location_cutout[3],\n",
    "        ] = 0\n",
    "\n",
    "    # get labels of connected components\n",
    "    labels = measure.label(thresh, connectivity=2, background=0)\n",
    "\n",
    "    if PLOTS: \n",
    "        full_image_thresh_fig, full_image_thresh_ax = plt.subplots(1, 4, figsize=(21, 9))\n",
    "        full_image_thresh_ax[0].imshow(thresh)\n",
    "        full_image_thresh_ax[0].title.set_text('global threshold')\n",
    "        full_image_thresh_ax[1].imshow(ad_thresh)\n",
    "        full_image_thresh_ax[1].title.set_text('adaptive threshold')\n",
    "        full_image_thresh_ax[2].imshow(img)\n",
    "        full_image_thresh_ax[2].title.set_text('original rgb')\n",
    "        full_image_thresh_ax[3].imshow(labels)\n",
    "        full_image_thresh_ax[3].title.set_text('labels')\n",
    "        plt.show()              \n",
    "        # plt.savefig(\"test.png\")\n",
    "\n",
    "    # Save the labels as a jpg for the full image\n",
    "    if args.save_full_mask_dir is not None:\n",
    "        args.save_full_mask_dir.mkdir(parents=True, exist_ok=True)\n",
    "        cv2.imwrite(\n",
    "            str(args.save_full_mask_dir / f\"labels_{fo.stem}.jpg\").lower(),\n",
    "            (labels).astype(np.uint8),\n",
    "        )\n",
    "        if not cfg.impa_save_clips_plus_features:\n",
    "            if args.verbose:\n",
    "                print(\"skipping clip generation\")\n",
    "            continue\n",
    "\n",
    "    # get region properties\n",
    "    rprop = measure.regionprops(labels)\n",
    "    mask = np.ones(thresh.shape, dtype=\"uint8\")\n",
    "\n",
    "    # init some stuff\n",
    "    sub_df = pd.DataFrame([])\n",
    "    c = 1\n",
    "    # loop through identified regions and get some properties\n",
    "    for label in range(len(rprop)):  # np.unique(labels):\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        reg_pro = rprop[label]\n",
    "\n",
    "        # skip background\n",
    "        if reg_pro.label == 0:\n",
    "            continue\n",
    "\n",
    "        # skip small objects\n",
    "        if reg_pro.area < cfg.impa_area_threshold:  # 5000 defauilt\n",
    "            continue\n",
    "\n",
    "        # get mask for current region of interest\n",
    "        current_mask = np.zeros(thresh.shape)\n",
    "        current_mask[labels == reg_pro.label] = 1\n",
    "\n",
    "        # coordinates of bounding box corners for current region of interest\n",
    "        (\n",
    "            min_row,\n",
    "            min_col,\n",
    "            max_row,\n",
    "            max_col,\n",
    "        ) = reg_pro.bbox  # cv2.boundingRect(approx)\n",
    "        (x, y, w, h) = (min_col, min_row, max_col - min_col, max_row - min_row)\n",
    "\n",
    "        # get the bounding box with some buffer\n",
    "        (x_e, y_e, w_e, h_e) = (\n",
    "            np.max((x - cfg.impa_bounding_box_buffer, 0)),\n",
    "            np.max((y - cfg.impa_bounding_box_buffer, 0)),\n",
    "            w + 2 * cfg.impa_bounding_box_buffer,\n",
    "            h + 2 * cfg.impa_bounding_box_buffer,\n",
    "        )\n",
    "\n",
    "        if PLOTS:            \n",
    "            clip_crop_fig, clip_crop_ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "            clip_crop_ax.imshow(img[:, :, [0, 1, 2]], aspect=\"auto\")\n",
    "            rect = plt.Rectangle(\n",
    "                (x_e, y_e), w_e, h_e, fc=\"none\", ec=\"black\", linewidth=2\n",
    "            )\n",
    "            clip_crop_ax.add_patch(rect)\n",
    "            \n",
    "            # clear_output(wait = True)\n",
    "            display(full_image_thresh_fig)\n",
    "            \n",
    "            plt.show()\n",
    "            # plt.savefig(f\"test_mask{c}.png\")\n",
    "            # exit()\n",
    "\n",
    "        # get the crop of the image and the mask\n",
    "        crop = img[y_e : y_e + h_e, x_e : x_e + w_e, [2, 1, 0]]\n",
    "        crop_hsv = hsv[y_e : y_e + h_e, x_e : x_e + w_e, :]\n",
    "        crop_mask = current_mask[y_e : y_e + h_e, x_e : x_e + w_e]\n",
    "        crop_im_t = im_t[y_e : y_e + h_e, x_e : x_e + w_e]\n",
    "\n",
    "        im_crop_m = crop.reshape(-1, 3)[\n",
    "            crop_mask.reshape(\n",
    "                -1,\n",
    "            ).astype(bool),\n",
    "            :,\n",
    "        ]\n",
    "        hsv_crop_m = crop_hsv.reshape(-1, 3)[\n",
    "            crop_mask.reshape(\n",
    "                -1,\n",
    "            ).astype(bool),\n",
    "            :,\n",
    "        ]\n",
    "\n",
    "        # save actual image and mask crops\n",
    "        # Avoid \"invalid value encountered in true_divide\" warning\n",
    "        np.seterr(divide=\"ignore\", invalid=\"ignore\")\n",
    "        cv2.imwrite(\n",
    "            str(outdir / (f\"{fo.stem}_{c}_mask.{cfg.impa_image_format}\").lower()),\n",
    "            (255 * crop_mask / crop_mask).astype(np.uint8),\n",
    "            [cv2.IMWRITE_JPEG_QUALITY, 100],\n",
    "        )\n",
    "\n",
    "        # reactivate warnings\n",
    "        np.seterr(divide=\"warn\", invalid=\"warn\")\n",
    "\n",
    "        cv2.imwrite(\n",
    "            str(outdir / (f\"{fo.stem}_{c}_rgb.{cfg.impa_image_format}\").lower()),\n",
    "            crop,\n",
    "            [cv2.IMWRITE_JPEG_QUALITY, 100],\n",
    "        )\n",
    "        # get average color of the crop\n",
    "        # not really needed, aren't they\n",
    "        # im_crop_cmean = str(np.mean(im_crop_m, axis=0))\n",
    "        # hsv_crop_cmean = str(np.mean(hsv_crop_m, axis=0))\n",
    "\n",
    "        # im_crop_std = str(np.std(im_crop_m, axis=0))\n",
    "        # hsv_crop_std = str(np.std(hsv_crop_m, axis=0))\n",
    "\n",
    "        mask = mask + current_mask * c\n",
    "\n",
    "        if PLOTS:\n",
    "            clip_fig, clip_ax = plt.subplots(1, 4, figsize=(10, 6))\n",
    "            clip_ax[0].imshow(crop)\n",
    "            clip_ax[0].title.set_text('crop')\n",
    "            clip_ax[1].imshow(reg_pro.image)  # crop_mask)\n",
    "            clip_ax[1].title.set_text('binary mask')\n",
    "            clip_ax[2].imshow(\n",
    "                (\n",
    "                    crop * np.transpose(np.tile(crop_mask, (3, 1, 1)), (1, 2, 0))\n",
    "                ).astype(np.uint8)\n",
    "            )\n",
    "            clip_ax[2].title.set_text('mask HSV')\n",
    "            im_t_crop_m = crop_im_t.reshape(-1, 1)[\n",
    "                crop_mask.reshape(\n",
    "                    -1,\n",
    "                ).astype(bool),\n",
    "                :,\n",
    "            ]\n",
    "            clip_ax[3].hist(im_t_crop_m, bins=50)\n",
    "            clip_ax[3].title.set_text('colour histogram')\n",
    "            # plt.pause(1)\n",
    "            plt.show()                 \n",
    "\n",
    "        sub_df = {}\n",
    "        sub_df[\"input_file\"] = raw_image_in\n",
    "        sub_df[\"species\"] = raw_image_in.name.split(\".\")[0]\n",
    "        sub_df[\"png_mask_id\"] = c\n",
    "        sub_df[\"reg_lab\"] = reg_pro.label\n",
    "        sub_df[\"squareness\"] = w / float(h)\n",
    "        # sub_df[\"average_color\"] = im_crop_cmean\n",
    "        # sub_df[\"average_color_std\"] = im_crop_std\n",
    "        # sub_df[\"average_hsv\"] = hsv_crop_cmean\n",
    "        # sub_df[\"average_hsv_std\"] = hsv_crop_std\n",
    "        sub_df[\"tight_bb\"] = f\"({x}, {y}, {w}, {h})\"\n",
    "        sub_df[\"large_bb\"] = f\"({x_e}, {y_e}, {w_e}, {h_e})\"\n",
    "        sub_df[\"ell_minor_axis\"] = reg_pro.minor_axis_length\n",
    "        sub_df[\"ell_major_axis\"] = reg_pro.major_axis_length\n",
    "        sub_df[\"bbox_area\"] = reg_pro.bbox_area\n",
    "        sub_df[\"area_px\"] = reg_pro.area\n",
    "        sub_df[\"mask_centroid\"] = str(reg_pro.centroid)\n",
    "        sub_df = pd.DataFrame(data=sub_df, index=[0])\n",
    "\n",
    "        mask_props.append(sub_df)\n",
    "        c += 1\n",
    "        \n",
    "if not PLOTS:\n",
    "    if mask_props:\n",
    "        mask_props = pd.concat(mask_props).reset_index().drop(columns=[\"index\"])\n",
    "        mask_props.to_csv(outdir / \"_mask_properties.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mzbfull2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
