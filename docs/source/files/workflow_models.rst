Workflow and Models
###################
Here we illustrate the main functionalities of the modules and information about the models. 
We also show the usage of ``workflows`` and its ``.sh`` files. 

.. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Introduction
============

This project contains 3 modules: 

# - :ref:`files/scripts/processing_scripts:Segmentation`: These are mainly used as preprocessing steps for classification and measurements, but are handy functionalities on their own. The raw input images are wide RGB photographs of many macrozoobenthos samples, which are hard to process on their own. A first step is to detect all the insects as independent and disjoint objects, and to crop them out of the original image. This is done with traditional image processing methods, and it is completely unsupervised. From this, we derive i) an RGB crop of the organism, with filename corresponding to the original image name plus clip ID and ii) a binary mask of the insect, filename corresponings to the original image name plus clip ID. The binary mask can be used to compute the area of the insect (number of pixels) and RGB + clip can be used to compute local descriptors. 
.. (we can think of using classificaiton models that are not convolutional neural nets, such as random forests, to classify the insect based on these local color and shape descriptors).

# - :ref:`files/scripts/processing_scripts:Skeleton`: this repo contains the code to measure the length and width of macrozoobenthos samples. The user can chose two approaches: one is completely unsupervised, and uses image filtering, mathematical morphology and graph-traversal algorithm to come up with a measure approximating *only the length* of the sample. Performance is better for samples that are long and thin, and slightly less accurate for complex shapes. The approximation for this approach is made from a binary mask obtained automatically from the MZB sample. 
The second approach to measure size is based on supervised deep learning models, which are trained based on some manual annotations of insects length (head to tail) and head width (width of head). Those annotations were provided by a human expert. The scripts in this repo allow to finetune this model, and to use this model for inference on new images. The two predictions of this model correspond to the probability of each pixel to be along the "body" skeleton segments, or along the "head" skeleton segments. Postprocessing thins those predictions to a line, and the size of the sample is then approximated by the sum of the length of the body and the head.

# - :ref:`files/scripts/processing_scripts:Classification`: this repo contains the code to train and test a model to classify the macrozoobenthos samples, according to a a set of predefined classes. In our experiments, image classes were specified in the filename, but this can be changed to a more flexible approach. We reclassify class names thanks to the ``data/MZB_taxonomy.csv`` file, which groups classes according to a taxonomic hierarchy. The data used to fine tune pretrained deep learning classifiers is then duplicated according to our structure (each class in its own folder) in ``data/learning_sets/project_portable_flume/aggregated_learning_sets/``.

.. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Working with the project
========================

Project structure
~~~~~~~~~~~~~~~~~

 - ``data/``: contains the raw data, and the some of the data generated by scripts, that is then reused elsewhere (eg image clips -- aka blobs, masks, skeletons, etc). Some CSV with derived measures and features are momentarily stored there but might not always be needed. We can maybe generate those conditionally by addition option in ``configs/global_configuration.yaml``.
 - ``mzb_workflow/``: contains the utility functions for each module, that one can modify for specific needs, but are in general good and generic enough to be used as it is.
 - ``scripts/``: contains the scripts to run the different modules. Each module has its own folder, and each script is named after the module it implements. These modules can _be run in isolation_, ie, each one of those can be run independently from the others from the CLI. Arguments to these scripts are only the paths to the input and output folders, and the name of the project, model names. This is made so renku workflow can track main inputs and outputs (TODOs) without making it complex. A user might just need to specify folder names of new data on which to perform inference, and that's all.  Some work might be needed to ensure that they can also be run interactively, but for this, it is better to duplicate those files in a ``notebooks/`` folder, and to run them interactively from there after modding them. 
 - ``models/``: contains the pretrained models used in the different modules. The models are first downloaded from pytorch model zoo, then finetuned. The finetuned models are saved in this folder.
 - ``configs/``: contains for now one big config file, ``config.yaml``, which contains all the parameters for the different modules. This module contains all defaults settings and hyperparameters, and it can be modified by the user to change the behavior of the scripts. Maybe good idea to create copies of this config, with per-experiment naming, or create a branch of the repo, etc. Make sure to version those also, with good commit names. Those config files are always input to main scripts. 
 - ``results/``: contains the results of the different modules. Each module has its own folder, and each script is named after the module it implements. It probably needs to be better structured and organized. 
 - ``workflows/``: will contain the renku workflows, for now contains just an implementation of the serial pipeline in bash scripts. One for the inference pipeline, and two for finetuning the classification and size measurement models.

Workflow files
~~~~~~~~~~~~~~

In this folder, there are "workflow" files that can be used to run the pipeline. Those files are nothing else that a chain of python commands implementing the flow of the processing pipeline. For instance, just run ``./workflows/run_finetune_skeletonization.sh`` to fine tune the skeletonization model. 

TODO: transfer those bash scripts to renku workflows, so that the renku can track the dependencies and the inputs and outputs of each step, and generate the graph of the workflow.

Changing interactive session dependencies
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The full conda env is given in ``environment.yml``. We need to check if the docker image builds...

Weight and biases
~~~~~~~~~~~~~~~~~

When training models, logging is made throught weight and biases automatically. Refer to the documentation of wandb for more details, at [https://docs.wandb.ai/](https://docs.wandb.ai/). You will need to create an account and to login to wandb to be able to log the training. 

After installing all requirements, run ``wandb login``.

.. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Available models
================
So far, these architectures are available for classification: 

- ``vgg``: VGG 16
- ``resnet18``: ResNet 18 layers
- ``resnet50``: ResNet 50 layers
- ``densenet161``: DenseNet 161 layers
- ``mobilenet``: MobileNet V2 
- ``efficientnet-b2``: EfficientNet B2
- ``efficientnet-b1``: EfficientNet B1
- ``vit16``: Vision Transformer 16 
- ``convnext-small``: ConvNext Small

The models are pre-trained on ImageNet and can be downloaded from the PyTorch model zoo. We use ``torchvision.models`` to load the models, and we pass ``weights={ModelName}_Weigths.IMAGENET1K_V1`` for the pre-trained weights. This can be changed depending on needs. 

Adding a new model
------------------
In ``mzbsuite/utils.py`` you can either add a case to the function ``read_pretrained_model(architecture, n_class)`` or add a function returning a pytorch model. In general, the layers of these classifiers are all frozen and only the last fully connected layers are trained on the annotated data. This seemed to work in most of our cases, but can be changed in a simple way in the function. 
